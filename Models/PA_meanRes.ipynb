{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import kagglegym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import ensemble, linear_model, metrics\n",
    "\n",
    "env = kagglegym.make()\n",
    "o = env.reset()\n",
    "train = o.train\n",
    "print(train.shape)\n",
    "d_mean= train.median(axis=0)\n",
    "train[\"nbnulls\"]=train.isnull().sum(axis=1)\n",
    "col=[x for x in train.columns if x not in ['id', 'timestamp', 'y']]\n",
    "\n",
    "rnd=17\n",
    "\n",
    "#keeping na information on some columns (best selected by the tree algorithms)\n",
    "add_nas_ft=True\n",
    "nas_cols=['technical_9', 'technical_0', 'technical_32', 'technical_16', 'technical_38', \n",
    "'technical_44', 'technical_20', 'technical_30', 'technical_13']\n",
    "#columns kept for evolution from one month to another (best selected by the tree algorithms)\n",
    "add_diff_ft=True\n",
    "diff_cols=['technical_22','technical_20', 'technical_30', 'technical_13', 'technical_34']\n",
    "\n",
    "#homemade class used to infer randomly on the way the model learns\n",
    "class createLinearFeatures:\n",
    "    \n",
    "    def __init__(self, n_neighbours=1, max_elts=None, verbose=True, random_state=None):\n",
    "        self.rnd=random_state\n",
    "        self.n=n_neighbours\n",
    "        self.max_elts=max_elts\n",
    "        self.verbose=verbose\n",
    "        self.neighbours=[]\n",
    "        self.clfs=[]\n",
    "        \n",
    "    def fit(self,train,y):\n",
    "        if self.rnd!=None:\n",
    "            random.seed(self.rnd)\n",
    "        if self.max_elts==None:\n",
    "            self.max_elts=len(train.columns)\n",
    "        list_vars=list(train.columns)\n",
    "        random.shuffle(list_vars)\n",
    "        \n",
    "        lastscores=np.zeros(self.n)+1e15\n",
    "\n",
    "        for elt in list_vars[:self.n]:\n",
    "            self.neighbours.append([elt])\n",
    "        list_vars=list_vars[self.n:]\n",
    "        \n",
    "        for elt in list_vars:\n",
    "            indice=0\n",
    "            scores=[]\n",
    "            for elt2 in self.neighbours:\n",
    "                if len(elt2)<self.max_elts:\n",
    "                    clf=linear_model.LinearRegression(fit_intercept=False, normalize=True, copy_X=True, n_jobs=-1) \n",
    "                    clf.fit(train[elt2+[elt]], y)\n",
    "                    scores.append(metrics.mean_squared_error(y,clf.predict(train[elt2 + [elt]])))\n",
    "                    indice=indice+1\n",
    "                else:\n",
    "                    scores.append(lastscores[indice])\n",
    "                    indice=indice+1\n",
    "            gains=lastscores-scores\n",
    "            if gains.max()>0:\n",
    "                temp=gains.argmax()\n",
    "                lastscores[temp]=scores[temp]\n",
    "                self.neighbours[temp].append(elt)\n",
    "\n",
    "        indice=0\n",
    "        for elt in self.neighbours:\n",
    "            clf=linear_model.LinearRegression(fit_intercept=False, normalize=True, copy_X=True, n_jobs=-1) \n",
    "            clf.fit(train[elt], y)\n",
    "            self.clfs.append(clf)\n",
    "            if self.verbose:\n",
    "                print(indice, lastscores[indice], elt)\n",
    "            indice=indice+1\n",
    "                    \n",
    "    def transform(self, train):\n",
    "        indice=0\n",
    "        for elt in self.neighbours:\n",
    "            #this line generates a warning. Could be avoided by working and returning\n",
    "            #with a copy of train.\n",
    "            #kept this way for memory management\n",
    "            train['neighbour'+str(indice)]=self.clfs[indice].predict(train[elt])\n",
    "            indice=indice+1\n",
    "        return train\n",
    "    \n",
    "    def fit_transform(self, train, y):\n",
    "        self.fit(train, y)\n",
    "        return self.transform(train)\n",
    "\n",
    "#a home-made class attempt to remove outliers by successive quantization on residuals\n",
    "class recurrent_linear_approx():\n",
    "    def __init__(self, quant=.999, limit_size_train=.9):\n",
    "        self.quant=quant\n",
    "        self.limit_size_train=limit_size_train\n",
    "        self.bestmodel=[]\n",
    "       \n",
    "    def fit(self, train, y):\n",
    "        internal_model=linear_model.RidgeCV(fit_intercept=False)\n",
    "        bestscore=1e15\n",
    "        better=True\n",
    "        indextrain=train.dropna().index\n",
    "        limitlen=len(train)*self.limit_size_train\n",
    "        while better:\n",
    "            internal_model.fit(train.ix[indextrain], y.ix[indextrain])\n",
    "            score=metrics.mean_squared_error(internal_model.predict(train.ix[indextrain]), y.ix[indextrain])\n",
    "            if score < bestscore:\n",
    "                bestscore=score\n",
    "                self.bestmodel=internal_model\n",
    "                residual=y.ix[indextrain]-internal_model.predict(train.ix[indextrain])\n",
    "                indextrain=residual[abs(residual)<=abs(residual).quantile(self.quant)].index\n",
    "                if len(indextrain)<limitlen:\n",
    "                    better=False\n",
    "            else:\n",
    "                better=False\n",
    "                self.bestmodel=internal_model\n",
    "\n",
    "    def predict(self, test):\n",
    "        return self.bestmodel.predict(test)\n",
    "\n",
    "if add_nas_ft:\n",
    "    for elt in nas_cols:\n",
    "        train[elt + '_na'] = pd.isnull(train[elt]).apply(lambda x: 1 if x else 0)\n",
    "        #no need to keep columns with no information\n",
    "        if len(train[elt + '_na'].unique())==1:\n",
    "            print(\"removed:\", elt, '_na')\n",
    "            del train[elt + '_na']\n",
    "            nas_cols.remove(elt)\n",
    "\n",
    "\n",
    "if add_diff_ft:\n",
    "    train=train.sort_values(by=['id','timestamp'])\n",
    "    for elt in diff_cols:\n",
    "        #a quick way to obtain deltas from one month to another but it is false on the first\n",
    "        #month of each id\n",
    "        train[elt+\"_d\"]= train[elt].rolling(2).apply(lambda x:x[1]-x[0]).fillna(0)\n",
    "    #removing month 0 to reduce the impact of erroneous deltas\n",
    "    train=train[train.timestamp!=0]\n",
    "\n",
    "print(train.shape)\n",
    "cols=[x for x in train.columns if x not in ['id', 'timestamp', 'y']]\n",
    "\n",
    "\n",
    "#generation of linear models\n",
    "cols2fit=['technical_22','technical_20', 'technical_30_d', 'technical_20_d', 'technical_30', \n",
    "'technical_13', 'technical_34']\n",
    "models=[]\n",
    "columns=[]\n",
    "residuals=[]\n",
    "for elt in cols2fit:\n",
    "    print(\"fitting linear model on \", elt)\n",
    "    model=recurrent_linear_approx(quant=.995, limit_size_train=.9)\n",
    "    model.fit(train.loc[:,[elt]],train.loc[:, 'y'])\n",
    "    models.append(model)\n",
    "    columns.append([elt])\n",
    "    residuals.append(abs(model.predict(train[[elt]].fillna(d_mean))-train.y))\n",
    "\n",
    "train=train.fillna(d_mean)\n",
    "    \n",
    "#adding all trees generated by a tree regressor\n",
    "print(\"adding new features\")\n",
    "featureexpander=createLinearFeatures(n_neighbours=30, max_elts=2, verbose=True, random_state=rnd)\n",
    "index2use=train[abs(train.y)<0.086].index\n",
    "featureexpander.fit(train.ix[index2use,cols],train.ix[index2use,'y'])\n",
    "trainer=featureexpander.transform(train[cols])\n",
    "treecols=trainer.columns\n",
    "\n",
    "print(\"training trees\")\n",
    "model = ensemble.ExtraTreesRegressor(n_estimators=100, max_depth=4, n_jobs=-1, random_state=rnd, verbose=0)\n",
    "model.fit(trainer,train.y)\n",
    "print(pd.DataFrame(model.feature_importances_,index=treecols).sort_values(by=[0]).tail(30))\n",
    "for elt in model.estimators_:\n",
    "    models.append(elt)\n",
    "    columns.append(treecols)\n",
    "    residuals.append(abs(elt.predict(trainer)-train.y))\n",
    "\n",
    "\n",
    "#model selection : create a new target selecting models with lowest asolute residual for each line\n",
    "#the objective at this step is to keep only the few best elements which should\n",
    "#lead to a better generalization\n",
    "num_to_keep=11\n",
    "targetselector=np.array(residuals).T\n",
    "#targetselector=np.argmin(targetselector, axis=1)\n",
    "targetselector=(np.mean(targetselector,axis=0)) ####\n",
    "#print(\"selecting best models:\")\n",
    "#print(pd.Series(targetselector).value_counts().head(num_to_keep))\n",
    "\n",
    "#tokeep=pd.Series(targetselector).value_counts().head(num_to_keep).index\n",
    "tokeep=targetselector.argsort()[:(num_to_keep)] ####\n",
    "tokeepmodels=[]\n",
    "tokeepcolumns=[]\n",
    "tokeepresiduals=[]\n",
    "for elt in tokeep:\n",
    "    tokeepmodels.append(models[elt])\n",
    "    tokeepcolumns.append(columns[elt])\n",
    "    tokeepresiduals.append(residuals[elt])\n",
    "\n",
    "#creating a new target for a model in charge of predicting which model is best for the current line\n",
    "targetselector=np.array(tokeepresiduals).T\n",
    "targetselector=np.argmin(targetselector, axis=1)\n",
    "\n",
    "print(\"training selection model\")\n",
    "modelselector = ensemble.ExtraTreesClassifier(n_estimators=100, max_depth=4, n_jobs=-1, random_state=rnd, verbose=0)\n",
    "modelselector.fit(trainer, targetselector)\n",
    "print(pd.DataFrame(modelselector.feature_importances_,index=treecols).sort_values(by=[0]).tail(30))\n",
    "\n",
    "#lastvalues=train[train.timestamp==905][['id']+diff_cols].copy()\n",
    "lastvalues=train[train.timestamp==train.timestamp.iloc[-1]][['id']+diff_cols].copy()####\n",
    "\n",
    "print(\"end of trainind, now predicting\")\n",
    "indice=0\n",
    "countplus=0\n",
    "rewards=[]\n",
    "while True:\n",
    "    indice+=1\n",
    "    test = o.features\n",
    "    test[\"nbnulls\"]=test.isnull().sum(axis=1)\n",
    "    if add_nas_ft:\n",
    "        for elt in nas_cols:\n",
    "            test[elt + '_na'] = pd.isnull(test[elt]).apply(lambda x: 1 if x else 0)\n",
    "    test=test.fillna(d_mean)\n",
    "    d_mean=test.median(axis=0)####\n",
    "\n",
    "    pred = o.target\n",
    "    if add_diff_ft:\n",
    "        #creating deltas from lastvalues\n",
    "        indexcommun=list(set(lastvalues.id) & set(test.id))\n",
    "        lastvalues=pd.concat([test[test.id.isin(indexcommun)]['id'],\n",
    "            pd.DataFrame(test[diff_cols][test.id.isin(indexcommun)].values-lastvalues[diff_cols][lastvalues.id.isin(indexcommun)].values,\n",
    "            columns=diff_cols, index=test[test.id.isin(indexcommun)].index)],\n",
    "            axis=1)\n",
    "        #adding them to test data    \n",
    "        test=test.merge(right=lastvalues, how='left', on='id', suffixes=('','_d')).fillna(0)\n",
    "        #storing new lastvalues\n",
    "        lastvalues=test[['id']+diff_cols].copy()\n",
    "    \n",
    "    testid=test.id\n",
    "    test=featureexpander.transform(test[cols])\n",
    "    #prediction using modelselector and models list\n",
    "    selected_prediction = modelselector.predict_proba(test.loc[: ,treecols])\n",
    "    for ind,elt in enumerate(tokeepmodels):\n",
    "        pred['y']+=selected_prediction[:,ind]*elt.predict(test[tokeepcolumns[ind]])\n",
    "\n",
    "    indexbase=pred.index\n",
    "    pred.index=testid    \n",
    "    oldpred=pred['y']\n",
    "    pred.index=indexbase\n",
    "    \n",
    "    o, reward, done, info = env.step(pred)\n",
    "    rewards.append(reward)\n",
    "    if reward>0:\n",
    "        countplus+=1\n",
    "    \n",
    "    if indice%100==0:\n",
    "        print(indice, countplus, reward, np.mean(rewards))\n",
    "        \n",
    "    if done:\n",
    "        print(info[\"public_score\"])\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
